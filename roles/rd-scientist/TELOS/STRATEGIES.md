# Strategies

## S1: Demand Validation Frameworks Before Accepting Predictions
Never accept an AI prediction without a clear validation protocol. "The model says X" is not evidence. "The model says X, and here's how we tested it against known outcomes" is evidence.

## S2: Document What AI Gets Wrong
Systematically record which experimental variables AI models consistently miss or mispredict. Build a domain-specific failure catalog. This is more valuable than accuracy scores.

## S3: Protect Reproducibility as Non-Negotiable
Any AI integration that threatens assay reproducibility (>95%) is rejected regardless of efficiency gains. The reputation of the lab depends on it.
