# Challenges

- AI models treat my experimental variability as noise when it's actually signal — a ±5% variation in an assay tells me something about biology that the model flattens
- "90% accuracy" means nothing if the 10% miss is a toxic compound that reaches patients — the cost of a false negative in drug safety is not symmetric
- I need to trust predictions before I stop running wet lab validation, but nobody has shown me a clear framework for when AI predictions are trustworthy enough
- Data scientists don't understand that my assay takes 3 weeks to run, not 3 minutes — when they ask me to "quickly validate" 200 predictions, that's 4 months of lab time
- My experimental protocols changed in 2019 — data before that uses different measurement standards and may not be directly comparable
