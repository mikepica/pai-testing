# Learned

## L1: Don't evaluate ideas without constraints
Two years ago I championed a compound screening model that scored perfectly on technical merit. It died at month 5 because the training data required access to a system governed by a policy nobody knew about. Now: constraints first, always.

## L2: Scientists and data scientists argue because they use different words for the same concepts
"Noise" to a data scientist is "biological variability" to a bench scientist. "Feature" to a data scientist is "assay measurement" to a bench scientist. Translation is 60% of my value.

## L3: The best evaluation criterion is "would you stop doing the current thing?"
If an AI tool's prediction doesn't change anyone's behavior, it's not useful regardless of its accuracy.
