---
capture_type: LEARNING
timestamp: 2026-02-08 13:28:18 UTC
auto_captured: true
tags: [auto-capture]
---

# Quick Learning: Task Completion

**Date:** 2026-02-08
**Auto-captured:** Yes

---

## Summary

N/A

## Analysis

N/A

## Actions Taken

N/A

## Results

N/A

## Current Status

N/A

## Next Steps

N/A

---

<details>
<summary>Full Response</summary>

â”â”â” ğŸ§  THINK â”â”â” 2/7

ğŸ” **THINKING TOOLS ASSESSMENT** (justify exclusion):
â”‚ Council:          EXCLUDE â€” single clear answer to deliver, no tradeoffs to debate
â”‚ RedTeam:          EXCLUDE â€” explaining tools, not making a claim to stress-test
â”‚ FirstPrinciples:  EXCLUDE â€” tools are already well-defined, no hidden assumptions
â”‚ Science:          EXCLUDE â€” not iterative/experimental
â”‚ BeCreative:       EXCLUDE â€” explanation benefits from clarity, not divergence

ğŸ” **SKILL CHECK** (validate hook hints against ISC):
â”‚ Hook suggested:   Algorithm agent
â”‚ ISC requires:     Clear explanation of PAI internals â€” no agent spawning needed
â”‚ Final skills:     None â€” direct explanation from loaded CORE context

ğŸ¯ **CAPABILITY SELECTION:**
â”‚ Skills:     None needed
â”‚ Thinking:   None included (educational explanation)
â”‚ Primary:    Direct response â€” CORE context already loaded with full tool definitions
â”‚ Pattern:    Specialist
â”‚ Sequence:   Direct explanation
â”‚ Rationale:  All 5 tools are fully defined in the loaded CORE; no external research or agent delegation needed

â”â”â” ğŸ“‹ PLAN â”â”â” 3/7

Walk through each tool with: **what it is â†’ how it works mechanically â†’ when it fires â†’ the example from their table**. Then explain the overarching justify-exclusion principle and how tools layer on top of capability agents.

â”â”â” ğŸ”¨ BUILD + âš¡ EXECUTE â”â”â” 4-5/7

Here's how each thinking tool works in PAI:

---

## The 5 Thinking Tools

These are **meta-cognitive layers** â€” they don't replace the capability agents (Engineer, Architect, etc.) but shape *how the Algorithm thinks* before those agents execute.

### 1. Council â€” Multi-Agent Debate
**What it does:** Spawns 3-7 agents with distinct perspectives who debate the problem. Each agent argues for a different approach, they challenge each other, and a synthesis emerges.

**When to INCLUDE:** Multiple valid paths exist and you need to weigh tradeoffs â€” not just pick the obvious one.

**Your table's reasoning:** Three viable career paths (Innovation Lead vs. Solutions Architect vs. AI PM) with no clear winner. Council debates the tradeoffs between them rather than me arbitrarily picking one.

---

### 2. FirstPrinciples â€” Deconstruct â†’ Challenge â†’ Reconstruct
**What it does:** Takes a problem apart to its fundamental truths. Three steps:
1. **Deconstruct** â€” Break the concept into its atomic components
2. **Challenge** â€” Question every assumption ("Why does it have to be this way?")
3. **Reconstruct** â€” Build back up from only the validated foundations

**When to INCLUDE:** The problem might be a symptom of something deeper, or rests on unexamined assumptions.

**Your table's reasoning:** "Enterprise AI adoption catalyst" is a compound concept. FirstPrinciples decomposes what that *actually requires* at its root â€” rather than accepting the label at face value.

---

### 3. RedTeam â€” Adversarial Stress-Testing (32 Agents)
**What it does:** Spawns up to 32 adversarial agents who try to *break* a proposal. They find holes, attack assumptions, identify failure modes, and surface risks the builder didn't see.

**When to INCLUDE:** You have a plan, claim, or design that needs stress-testing before committing.

**Your table's reasoning:** EXCLUDED because you're still *building* the plan â€” there's nothing concrete to attack yet. RedTeam is for *after* you have a proposal to defend.

---

### 4. Science â€” Hypothesis â†’ Test â†’ Analyze Cycles
**What it does:** Frames the problem as a scientific experiment. You form a hypothesis, design a test, run it, analyze results, and iterate. Multiple cycles until you converge on truth.

**When to INCLUDE:** The answer requires experimentation â€” debugging, performance optimization, A/B testing, or situations where you genuinely don't know the outcome until you try.

**Your table's reasoning:** EXCLUDED because career path analysis isn't iterative/experimental. You're not running tests â€” you're reasoning about options.

---

### 5. BeCreative â€” Extended Thinking + 5 Diverse Options
**What it does:** Activates extended/deep thinking mode and forces generation of 5 meaningfully different options. Prevents anchoring on the first idea and explores the full solution space.

**When to INCLUDE:** You need creative divergence â€” novel approaches, avoiding the obvious, or when the problem space is wide open.

**Your table's reasoning:** EXCLUDED because once a role is chosen (via Council), the requirements become clear. No need for creative divergence at that point.

---

## The Justify-Exclusion Principle

The key design principle: **tools are opt-OUT, not opt-IN.**

For every FULL-depth request, the Algorithm *must evaluate all 5 tools* and provide a reason for excluding each one. The burden of proof is on *not* using them.

This inverts the natural default. Without this rule, the Algorithm would gravitate toward familiar patterns (just use Engineer, skip everything else). The justify-exclusion principle forces genuine consideration of whether multi-perspective debate, root-cause analysis, or adversarial testing would improve the output.

**Invalid exclusion reasons:**
- "Too simple" â€” simple tasks can have hidden assumptions (FirstPrinciples)
- "Already know the answer" â€” confidence without verification is the failure mode (RedTeam)
- "Would take too long" â€” latency doesn't justify skipping quality

---

## How Tools Layer with Capabilities

```
Thinking Tools (meta-cognitive)     â† Council, FirstPrinciples, etc.
        â†“ shape
Capability Agents (execution)       â† Engineer, Architect, QA, etc.
        â†“ produce
Artifacts (output)                  â† Code, designs, plans, etc.
        â†“ verified against
ISC Criteria (success measures)     â† TaskCreate/TaskUpdate
```

Thinking tools run in the **THINK phase** and influence which capabilities get selected and how they execute. They're the "how should we think about this?" layer before the "who does the work?" layer.

---

â”â”â” âœ… VERIFY â”â”â” 6/7

</details>
